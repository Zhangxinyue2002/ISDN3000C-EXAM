# 从零学会白盒与黑盒：手把手学习指南

适合“上课没听懂，但想迅速搞明白”的你。全程中文，配最小例子 + 小练习 + 速查链接。

参考资料（本仓库）：
- 双语大纲与详细解释：`exam-cheatsheet.bilingual.md`
- 一页速记：`exam-cram-sheet.md`

—— 如果你只剩几小时，先看一页速记；如果你想从头到尾真正学会，按本文一步步做。

---

## 目标与结果
- 听得懂并能解释：白盒（Linear/Logistic、Decision Tree）与黑盒（MLP、CNN）的直觉、公式与使用场景。
- 会手算关键指标：MSE、交叉熵、Entropy/Information Gain、Precision/Recall/F1。
- 会动手跑一个迷你实验，能比较白盒 vs 黑盒的优缺点，并写出1段简短结论。

## 推荐学习节奏
- 7 天速成版（每天 60–90 分钟）
  1) 线性回归 + MSE；2) 逻辑回归 + BCE；3) 决策树 + 信息增益；
  4) 指标与混淆矩阵；5) 感知机→MLP + 反向传播直觉；
  6) CNN 基础（卷积/步幅/填充/池化）；7) 2 小时迷你项目。
- 14 天稳扎稳打（每天 40–60 分钟）：将上面每一天拆成两半，并增加1–2道练习。

## 前置知识（轻量复习）
- 线代：向量点积 w·x、矩阵乘法 XW；
- 微积分：对加权求和的导数、链式法则（知道“先里后外”）；
- 概率：二分类概率、对数与 log(乘积)=和 的性质。

---

## 白盒篇（可解释模型）

### 1) 线性回归（Linear Regression）与 MSE
- 直觉：画一条线 ŷ = w·x + b，让“预测误差平方和”最小。
- 公式：MSE = (1/n) Σ (y − ŷ)^2，误差大要被“平方”放大。
- 最小例子（1 分钟）：
  - 数据 (x,y)=(1,2),(2,3)；若 ŷ=x+0 → 误差 [1,1] → 平方 [1,1] → MSE=1；
  - 上移 1：ŷ=x+1 → 误差 [0,0] → MSE=0（在玩具例中完美）。
- 小练习：随机写 3 个点，假设一条线，手算 MSE；再调整 b 看 MSE 变化。
- 常见坑：
  - 忘“平方”致正负误差抵消；
  - 特征尺度差异大导致训练不稳（需归一化/标准化）；
  - 异常值主宰 MSE，考虑 MAE/Huber。

### 2) 逻辑回归（Logistic Regression）与交叉熵（BCE）
- 直觉：用 S 形函数 p=σ(z)=1/(1+e^(−z)) 输出“属于正类的概率”。
- 损失：BCE = −(1/n) Σ [ y ln p + (1−y) ln(1−p) ]，预测越接近标签，损失越小。
- 最小例子：
  - 对 y=1 的样本，若模型给 p=0.9 → 损失 −ln(0.9)≈0.105；若 p=0.1 → 损失大 −ln(0.1)=2.303。
- 小练习：对 2 个样本（一个正类一个负类），试不同阈值（0.3/0.5/0.7）得到混淆矩阵与 Precision/Recall。
- 常见坑：
  - 把“概率”当“类别”用；
  - 类不平衡时只看准确率会被误导。

### 3) 决策树（Decision Tree）：Entropy 与 Information Gain
- 直觉：每次选择一个“最能区分类别”的特征做分裂。
- 公式：
  - 熵 H(S)= −Σ p(c) log₂ p(c)；
  - 信息增益 IG(S,A)= H(S) − Σ_v (|S_v|/|S|) H(S_v)。
- 最小例子：S={+3,−1} → H(S)=0.811；按 A 分成 S1={+2,−0} H=0、S2={+1,−1} H=1 → IG=0.811−(2/4·0+2/4·1)=0.311。
- 小练习：自己构造 4–6 个样本，计算两种分裂的 IG，选更大者。
- 常见坑：
  - 不剪枝易过拟合；
  - 连续特征需考虑阈值切分点；
  - 类别过多时优先考虑合并或信息增益率。

### 4) 评估指标与混淆矩阵
- 混淆矩阵：

          预测 +   预测 −
  实际 +    TP        FN
  实际 −    FP        TN

- 指标：Acc=(TP+TN)/总；Prec=TP/(TP+FP)；Rec=TP/(TP+FN)；F1=2PR/(P+R)。
- 小练习：给定 TP=20, FP=5, FN=10, TN=65，算 Acc/Prec/Rec/F1。
- 经验：
  - 医疗偏召回（漏诊代价高）；垃圾邮件偏精度（误杀代价高）。

---

## 黑盒篇（难解释但强大）

### 1) 感知机 → 多层感知机（MLP）
- 直觉：多层“加权求和 + 非线性激活”，可以逼近复杂函数。
- 常见激活：ReLU/LeakyReLU（缓解梯度消失）、Sigmoid/Tanh（易饱和）。
- 过拟合对策：L2/L1 正则化、Dropout、早停（Early Stop）。

### 2) 反向传播（Backprop）直觉
- 关键：损失对每层参数的“责任”（梯度）由链式法则传回去：
  - 线性层：dW = δ x^T，db = δ；
  - 中间层：δ_l = (W_{l+1}^T δ_{l+1}) ⊙ f'(z_l)。
- 经验：
  - 初始化与归一化影响训练稳定性；
  - 学习率过大会震荡，过小很慢；用学习率衰减或自适应优化器（如 Adam）。

### 3) CNN 基础：卷积/步幅/填充/池化
- 卷积输出尺寸：H_out = ⌊(H + 2P − K)/S⌋ + 1（宽同理）。
- 参数量：K×K×C_in×C_out + C_out（若含 bias）。
- 直觉 ASCII：

  输入 4×4，核 3×3，步幅 1，无填充（valid）
  3×3 核滑动：
  [1 1 1 1] → 卷完得到 2×2
  [1 1 1 1]
  [1 1 1 1]
  [1 1 1 1]

- 池化：2×2 stride=2 常用，无参数。作用：降采样、抗微小扰动。

---

## 2 小时迷你项目（白盒 vs 黑盒对比）
- 目标：在一个二分类小数据上，比较逻辑回归、决策树、两层 MLP 的性能与可解释性。
- 步骤建议：
  1) 划分训练/验证/测试（60/20/20）；
  2) 统一做特征缩放（对逻辑回归/MLP 有益）；
  3) 三个模型统一用验证集调参（树的最大深度/最小样本数；MLP 的隐藏单元/学习率）；
  4) 报告测试集 Acc/Prec/Rec/F1、ROC‑AUC；
  5) 写 3–5 句结论：哪个泛化更好？哪个更易解释？用什么场景？
- 记录模板（建议复制到你的笔记）：
  - 数据集与划分：
  - 特征处理：
  - 模型与超参：
  - 指标：Acc/Prec/Rec/F1/ROC‑AUC：
  - 结论：

---

## 自测清单
- 我能解释 MSE/MAE 区别，并举一个“异常值”的例子。
- 我能手算一个简单划分的 Entropy 与 IG，写出选哪个特征。
- 我知道 Precision/Recall 的取舍，并能给出阈值调整的直觉。
- 我能写出一层线性层的 dW/db 推导思路（dW=δx^T）。
- 我能算出一个卷积层的输出尺寸与参数量。

## 速查链接
- 详细解释与更多例子：`exam-cheatsheet.bilingual.md`
- 一页速记（打印首选）：`exam-cram-sheet.md`

---

## 常见问答（Q&A）
- Q：白盒 vs 黑盒到底怎么选？
  - A：先看“是否必须解释”。必须可解释 → 线/逻辑回归、决策树优先；追求极致性能/复杂模式 → MLP/CNN。也可以“先白盒打 baseline，再上黑盒”。
- Q：为什么我训练不收敛？
  - A：先查学习率、特征尺度、激活函数是否饱和、是否泄露测试集、是否 batch 太大/太小。
- Q：指标到底报哪些？
  - A：类平衡时 Acc + F1；不平衡时 Prec/Rec/F1 + ROC‑AUC，且给出阈值或 PR 曲线。

---

## 下一步
- 30 分钟快速走一遍本文 + 打开 `exam-cram-sheet.md` 做口算训练；
- 预留 2 小时做“迷你项目”，写出你自己的 5 句结论；
- 考前一天，通读 `exam-cheatsheet.bilingual.md` 的“Beginner 模式”部分并默写关键公式。
